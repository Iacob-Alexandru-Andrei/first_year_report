{"rule":"SENTENCE_FRAGMENT","sentence":"^\\QWhile Federated Learning has enjoyed both an\\E$"}
{"rule":"TOO_LONG_SENTENCE","sentence":"^\\QThe study's goals would be to compare the final accuracy of each model at every level of the hierarchy on the edge models, the test set created from the proxy dataset, and an unseen validation set which differs substantially from both.\\E$"}
{"rule":"TOO_LONG_SENTENCE","sentence":"^\\QThe study's goals would be to compare the final accuracy of each model at every level of the hierarchy on the edge models, the test set created from the proxy dataset, and an unseen validation set which differs substantially from both.\\E$"}
{"rule":"TOO_LONG_PARAGRAPH","sentence":"^\\QThe performance on the proxy test-set and the hidden validations set\\E$"}
{"rule":"THREE_NN","sentence":"^\\QSince Fair Federated Learning attempts to construct a more uniform distribution accuracy distribution for the federated model over the local test sets of clients, the expectation was for it to either reduce the need for personalisation or to provide a better starting point from which to carry it out.\\E$"}
{"rule":"TOO_LONG_SENTENCE","sentence":"^\\QIt was a direct precursor to Bidirectional Hierarchical Federated Learning as it relied on a two-tiered model structure where each client trained both a group-level model and the global federated model using a mutual learning approach \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q based on knowledge distillation.\\E$"}
{"rule":"TOO_LONG_SENTENCE","sentence":"^\\QThis work proposes addressing the challenges above by constructing hierarchical tree-like federated network structures that allow bidirectional and potentially cyclical dataflow where each leaf is a client, and each internal node is a server capable of training on proxy public data.\\E$"}
{"rule":"PASSIVE_VOICE","sentence":"^\\Q“Residual” connections are not shown as they are not standard within t\\E$"}
{"rule":"PASSIVE_VOICE","sentence":"^\\Q“Residual” connections are not shown since they are constructed on an ad-hoc basis.\\E$"}
{"rule":"PASSIVE_VOICE","sentence":"^\\QIt assumes that the model initialization \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q, training \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q, and node aggregation \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q procedures are provided.\\E$"}
{"rule":"PASSIVE_VOICE","sentence":"^\\QAll variables are indexed on a per-node basis and assumed to be provided by the implementation.\\E$"}
{"rule":"PASSIVE_VOICE","sentence":"^\\QThe “residual” connections are defined as adjacency lists between nodes and their ancestors/descendents in \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q,\\E$"}
{"rule":"PASSIVE_VOICE","sentence":"^\\QAll variables are indexed per-node and assumed to be provided by the implementation.\\E$"}
{"rule":"THREE_NN","sentence":"^\\QTo further improve communication efficiency \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q propose a network and compute-aware resource allocation framework for hierarchical FL, which assigns clients to edge servers as to optimise costs.\\E$"}
{"rule":"THREE_NN","sentence":"^\\QThe benefits of the fully asynchronous approach are countermanded by both its sensitivity to client data heterogeneity and its inability to properly utilise cohort-based techniques such as Secure Aggregation \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q.\\E$"}
{"rule":"THREE_NN","sentence":"^\\QThe benefits of the fully asynchronous approach are countermanded by both its sensitivity to client data heterogeneity and its inability to properly utilize cohort-based techniques such as Secure Aggregation \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q.\\E$"}
{"rule":"THREE_NN","sentence":"^\\Q\\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q also show FedBuff results in a more uniform accuracy distribution over clients compared to oversampling since it incorporates updates from such stragglers rather than completely discarding their contribution.\\E$"}
{"rule":"TOO_LONG_SENTENCE","sentence":"^\\QSince DP comes with an inherent privacy-accuracy trade-off, the most relevant factor for its usability is whether an FL system can be scaled to operate over sufficiently large populations of clients as to allow productive training while offering a low \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q.\\E$"}
{"rule":"THREE_NN","sentence":"^\\QAfter near-convergence, it injects common personalisation regularises such as Knowledge Distillation \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q or Elastic-weight Consolidation \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q into the local client loss function where the reference model is taken to be the federated model from the start of the round.\\E$"}
{"rule":"TOO_LONG_SENTENCE","sentence":"^\\QThis allows the model to learn from the distributions of highly heterogeneous clients without harming performance on the overall federated network which enables a better distribution of accuracy over clients without the harm to average performance that Fair FL is known to bring \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q.\\E$"}
{"rule":"TOO_LONG_SENTENCE","sentence":"^\\QBased on these foundations, I outline the research directions B-HFL enables in \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q, which will be explored in the rest of my PhD. Finally, I provide a summary of the work I have already completed in \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q and provide a detailed timeline for the rest of my PhD in \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q.\\E$"}
{"rule":"THREE_NN","sentence":"^\\QIn such solutions, the distance function between models will be either the \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q norm of model differences or a similarity metric like the cosine similarity computer over flattened parameters.\\E$"}
{"rule":"THREE_NN","sentence":"^\\QTo further improve communication efficiency \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q propose a resource allocation framework for hierarchical FL, which assigns clients to edge servers to optimise costs.\\E$"}
{"rule":"TOO_LONG_SENTENCE","sentence":"^\\QThe work of \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q combines hierarchical aggregation and clustering in a mixed scenario of peer-to-peer and client-server FL where powerful clients take on the role of edge servers and perform aggregation before transmitting their models to the cloud for asynchronous aggregation.\\E$"}
{"rule":"THREE_NN","sentence":"^\\QTo further improve communication efficiency \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q propose a resource allocation framework which assigns clients to edge servers to optimise costs.\\E$"}
{"rule":"TOO_LONG_SENTENCE","sentence":"^\\QFedOPT \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q and Iterative Moving Averaging \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q are examples of a standard FL algorithms that offer unique implicit benefits for this hierarchical structure as they can maintain stateful accumulators and/or lists of previous models that permit every server to be distinguished.\\E$"}
{"rule":"TOO_LONG_SENTENCE","sentence":"^\\QFedOPT \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q and Iterative Moving Averaging \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q are examples of a standard FL algorithms that offer unique implicit benefits for this hierarchical structure as they can maintain stateful accumulators and/or lists of previous models that permit every server to be distinguished.\\E$"}
{"rule":"TOO_LONG_SENTENCE","sentence":"^\\QFedOPT \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q and Iterative Moving Averaging \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q are examples of a standard FL algorithms that offer unique implicit benefits for this hierarchical structure as they can maintain stateful accumulators and lists of previous models, respective, that permit every server to be distinguished.\\E$"}
{"rule":"TOO_LONG_SENTENCE","sentence":"^\\QFedOPT \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q and Iterative Moving Averaging \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q are examples of a standard FL algorithms that offer unique implicit benefits for this hierarchical structure as they can maintain stateful accumulators and lists of previous models, respective, that permit every server to be distinguished.\\E$"}
{"rule":"TOO_LONG_SENTENCE","sentence":"^\\QMore complex procedures may combine the known hierarchical topology with similarity metrics to create a distance matrix between internal node models and perform message passing similarly to Graph Neural Networks, as proposed by \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q, in order to create model interpolations from more than two models \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q.\\E$"}
{"rule":"TOO_LONG_PARAGRAPH","sentence":"^\\QFinally,\\E$"}
{"rule":"TOO_LONG_SENTENCE","sentence":"^\\QWithin the scope of the whole PhD, B-HFL would be considered a success if aggregation based on the hierarchical structure with “residuals” and persistent models allows for the creation of either a more performant global model or very good cluster models due to inter-cluster communication.\\E$"}
{"rule":"TOO_LONG_SENTENCE","sentence":"^\\QWithin the scope of the whole PhD, B-HFL would be considered a success if aggregation based on the hierarchical structure with “residuals” and persistent models allows for the creation of either a more performant global model or very good cluster models due to inter-cluster communication.\\E$"}
{"rule":"TOO_LONG_SENTENCE","sentence":"^\\QWithin the scope of the whole PhD, B-HFL would be considered a success if aggregation based on the hierarchical structure with “residuals” and persistent models allows for creating either a more performant global model or very good cluster models due to inter-cluster communication.\\E$"}
{"rule":"TOO_LONG_SENTENCE","sentence":"^\\QWithin the scope of the whole PhD, B-HFL would be considered a success if aggregation based on the hierarchical structure with “residuals” and persistent models allows for creating either a more performant global model or excellent cluster models due to inter-cluster communication.\\E$"}
{"rule":"TOO_LONG_SENTENCE","sentence":"^\\QWithin the scope of the whole PhD, B-HFL would be considered a success if aggregation based on the hierarchical structure with “residuals” and persistent models allows for faster convergence to the global optimum due to avoiding the diminishing effects of large cohorts, creating a more performant global model, or constructing excellent cluster models due to inter-cluster communication.\\E$"}
{"rule":"TOO_LONG_SENTENCE","sentence":"^\\QPotential risks to the methodology include: (a) the continued proliferation of large models trained on public corpora, which are challenging to federate without extensive GPU resources being available to the clients, (b) a lack of support from major FL libraries for hierarchical FL, (c) an inability to improve upon standard FL and cluster FL approaches.\\E$"}
{"rule":"TOO_LONG_SENTENCE","sentence":"^\\QFor the second, if the proposed B-HFL structure is too rigid to manipulate and would be thus difficult to apply in a practical FL scenario, it would be preferable to refocus the work entirely on inter-cluster communication through a parent node where the depth of the hierarchy never exceeds 3.\\E$"}
{"rule":"TOO_LONG_SENTENCE","sentence":"^\\QFor the second, if the proposed B-HFL structure is too rigid to manipulate and would be thus difficult to apply in a practical FL scenario, it would be preferable to refocus the work entirely on inter-cluster communication through a parent node where the depth of the hierarchy never exceeds that of the example system in \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q.\\E$"}
{"rule":"TOO_LONG_PARAGRAPH","sentence":"^\\QFor the second, if the proposed B-HFL structure is too rigid to manipulate and would be thus difficult to apply in a practical FL scenario, it would be preferable to refocus the work entirely on inter-cluster communication through a parent node where the depth of the hierarchy never exceeds that of the example system in \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q.\\E$"}
{"rule":"TOO_LONG_SENTENCE","sentence":"^\\QWithin the scope of the whole PhD, B-HFL would be considered a success if aggregation based on the hierarchical structure with “residuals” and persistent models allows for faster convergence to the global optimum due to avoiding the diminishing effects of large cohorts, creating a more performant global model, or constructing excellent cluster models through inter-cluster communication.\\E$"}
{"rule":"TOO_LONG_SENTENCE","sentence":"^\\QWithin the scope of the whole PhD, B-HFL would be successful if its structure allows for faster convergence to the global optimum by avoiding the diminishing effects of large cohorts, creating a more performant global model, or constructing excellent cluster models through inter-cluster communication.\\E$"}
{"rule":"TOO_LONG_SENTENCE","sentence":"^\\QThe standard FL objective can be modelled as seen in \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q, where \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q is the federated objective, \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q is the client set, \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q is the model, and \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q is the loss of client \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q weighted by their fraction of the total number of examples \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q.\\E$"}
{"rule":"EN_COMPOUNDS","sentence":"^\\QOne exception is the weight-sharing iterative clustering algorithm proposed by \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q, which serves as a middle point between a two-layer hierarchical solution and multi-task learning with personalisation layers; however, it still suffers from the drawback of having to train \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q versions of the model on each client.\\E$"}
{"rule":"AUXILIARY_DO_WITH_INCORRECT_VERB_FORM","sentence":"^\\QSmaller cohorts for each edge-server avoids the issue of decreasing pseudo-gradients norms noticed by \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q, as does clustering clients during edge-server assignment.\\E$"}
{"rule":"AUXILIARY_DO_WITH_INCORRECT_VERB_FORM","sentence":"^\\QUsing small cohorts for edge-servers avoids the issue of decreasing pseudo-gradients norms noticed by \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q, as does clustering clients during edge-server assignment.\\E$"}
