{"rule":"SENTENCE_FRAGMENT","sentence":"^\\QWhile Federated Learning has enjoyed both an\\E$"}
{"rule":"TOO_LONG_SENTENCE","sentence":"^\\QThe study's goals would be to compare the final accuracy of each model at every level of the hierarchy on the edge models, the test set created from the proxy dataset, and an unseen validation set which differs substantially from both.\\E$"}
{"rule":"TOO_LONG_SENTENCE","sentence":"^\\QThe study's goals would be to compare the final accuracy of each model at every level of the hierarchy on the edge models, the test set created from the proxy dataset, and an unseen validation set which differs substantially from both.\\E$"}
{"rule":"TOO_LONG_PARAGRAPH","sentence":"^\\QThe performance on the proxy test-set and the hidden validations set\\E$"}
{"rule":"THREE_NN","sentence":"^\\QSince Fair Federated Learning attempts to construct a more uniform distribution accuracy distribution for the federated model over the local test sets of clients, the expectation was for it to either reduce the need for personalisation or to provide a better starting point from which to carry it out.\\E$"}
{"rule":"TOO_LONG_SENTENCE","sentence":"^\\QIt was a direct precursor to Bidirectional Hierarchical Federated Learning as it relied on a two-tiered model structure where each client trained both a group-level model and the global federated model using a mutual learning approach \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q based on knowledge distillation.\\E$"}
{"rule":"TOO_LONG_SENTENCE","sentence":"^\\QThis work proposes addressing the challenges above by constructing hierarchical tree-like federated network structures that allow bidirectional and potentially cyclical dataflow where each leaf is a client, and each internal node is a server capable of training on proxy public data.\\E$"}
{"rule":"PASSIVE_VOICE","sentence":"^\\Q“Residual” connections are not shown as they are not standard within t\\E$"}
{"rule":"PASSIVE_VOICE","sentence":"^\\Q“Residual” connections are not shown since they are constructed on an ad-hoc basis.\\E$"}
{"rule":"PASSIVE_VOICE","sentence":"^\\QIt assumes that the model initialization \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q, training \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q, and node aggregation \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q procedures are provided.\\E$"}
{"rule":"PASSIVE_VOICE","sentence":"^\\QAll variables are indexed on a per-node basis and assumed to be provided by the implementation.\\E$"}
{"rule":"PASSIVE_VOICE","sentence":"^\\QThe “residual” connections are defined as adjacency lists between nodes and their ancestors/descendents in \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q,\\E$"}
{"rule":"PASSIVE_VOICE","sentence":"^\\QAll variables are indexed per-node and assumed to be provided by the implementation.\\E$"}
{"rule":"THREE_NN","sentence":"^\\QTo further improve communication efficiency \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q propose a network and compute-aware resource allocation framework for hierarchical FL, which assigns clients to edge servers as to optimise costs.\\E$"}
{"rule":"THREE_NN","sentence":"^\\QThe benefits of the fully asynchronous approach are countermanded by both its sensitivity to client data heterogeneity and its inability to properly utilise cohort-based techniques such as Secure Aggregation \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q.\\E$"}
{"rule":"THREE_NN","sentence":"^\\QThe benefits of the fully asynchronous approach are countermanded by both its sensitivity to client data heterogeneity and its inability to properly utilize cohort-based techniques such as Secure Aggregation \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q.\\E$"}
{"rule":"THREE_NN","sentence":"^\\Q\\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q also show FedBuff results in a more uniform accuracy distribution over clients compared to oversampling since it incorporates updates from such stragglers rather than completely discarding their contribution.\\E$"}
{"rule":"TOO_LONG_SENTENCE","sentence":"^\\QSince DP comes with an inherent privacy-accuracy trade-off, the most relevant factor for its usability is whether an FL system can be scaled to operate over sufficiently large populations of clients as to allow productive training while offering a low \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q.\\E$"}
{"rule":"THREE_NN","sentence":"^\\QAfter near-convergence, it injects common personalisation regularises such as Knowledge Distillation \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q or Elastic-weight Consolidation \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q into the local client loss function where the reference model is taken to be the federated model from the start of the round.\\E$"}
{"rule":"TOO_LONG_SENTENCE","sentence":"^\\QThis allows the model to learn from the distributions of highly heterogeneous clients without harming performance on the overall federated network which enables a better distribution of accuracy over clients without the harm to average performance that Fair FL is known to bring \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q.\\E$"}
{"rule":"TOO_LONG_SENTENCE","sentence":"^\\QBased on these foundations, I outline the research directions B-HFL enables in \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q, which will be explored in the rest of my PhD. Finally, I provide a summary of the work I have already completed in \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q and provide a detailed timeline for the rest of my PhD in \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q.\\E$"}
{"rule":"THREE_NN","sentence":"^\\QIn such solutions, the distance function between models will be either the \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q norm of model differences or a similarity metric like the cosine similarity computer over flattened parameters.\\E$"}
