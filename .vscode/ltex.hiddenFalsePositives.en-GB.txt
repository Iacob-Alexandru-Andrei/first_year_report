{"rule":"SENTENCE_FRAGMENT","sentence":"^\\QWhile Federated Learning has enjoyed both an\\E$"}
{"rule":"TOO_LONG_SENTENCE","sentence":"^\\QThe study's goals would be to compare the final accuracy of each model at every level of the hierarchy on the edge models, the test set created from the proxy dataset, and an unseen validation set which differs substantially from both.\\E$"}
{"rule":"TOO_LONG_SENTENCE","sentence":"^\\QThe study's goals would be to compare the final accuracy of each model at every level of the hierarchy on the edge models, the test set created from the proxy dataset, and an unseen validation set which differs substantially from both.\\E$"}
{"rule":"TOO_LONG_PARAGRAPH","sentence":"^\\QThe performance on the proxy test-set and the hidden validations set\\E$"}
{"rule":"THREE_NN","sentence":"^\\QSince Fair Federated Learning attempts to construct a more uniform distribution accuracy distribution for the federated model over the local test sets of clients, the expectation was for it to either reduce the need for personalisation or to provide a better starting point from which to carry it out.\\E$"}
{"rule":"TOO_LONG_SENTENCE","sentence":"^\\QIt was a direct precursor to Bidirectional Hierarchical Federated Learning as it relied on a two-tiered model structure where each client trained both a group-level model and the global federated model using a mutual learning approachÂ \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q based on knowledge distillation.\\E$"}
{"rule":"TOO_LONG_SENTENCE","sentence":"^\\QThis work proposes addressing the challenges above by constructing hierarchical tree-like federated network structures that allow bidirectional and potentially cyclical dataflow where each leaf is a client, and each internal node is a server capable of training on proxy public data.\\E$"}
