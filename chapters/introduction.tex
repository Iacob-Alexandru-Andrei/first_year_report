
Federated Learning (FL) is a distributed Machine Learning (ML) paradigm allowing multiple clients to train a shared collaborative model without communicating private data. It was introduced by \citet{FedAvg} as a means of reducing communication costs and lessening the privacy concerns of storing sensitive data in a centralised location, following the principle of data minimisation outlined in the \citet{White_House_Report} privacy report. These properties have led to FL applications with large cohorts of small edge devices, e.g., mobile keyboard prediction~\citep{GoogleKeyboard} for Android phones, and settings with larger entities subject to privacy requirements, e.g., hospitals~\citep{FLmedicine}. \citet{AdvancedAndOpenProblems} distinguish them as cross-device and cross-silo FL\@.

The growth in the preponderance of Federated Learning since the publication of \citet{FedAvg} can be ascribed to two primary trends. First, an increase in the privacy requirements of consumers and legal frameworks has put pressure on technology companies. This pressure drove interest in privacy-preserving ML at major corporations such as Google~\citep{FedAvg,GoogleKeyboard,tensorflowfederated,PracticalPrivateFLkairouz21b}, Microsoft~\citep{FLINT,Flute}, Meta~\citep{PAPAYA,FedBuff}, and Apple~\citep{AppleFL}. Second, ML has extended to domains with strict privacy requirements such as healthcare~\citep{FLmedicine,FutureOfHealth,BigDataCancer}, Human Activity Recognition~(HAR)~\citep{HARusingFL_2018,ClusterFL} or collaborations between competing corporations~\citep{SustainableIncentive,TowardsFairPrivacyPreservingFL}. Moreover, the emergence of Large Language Models (LLMs)~\citep{OpportunitiesAndRisksLLM} has made accessing private language corpora advantageous, leading to the development of Federated Natural Language Processing (FedNLP)~\citep{FedNLP}. Similarly, the release of openly available LLM pre-trained weights~\citep{LLaMA} allows collaboration between entities with low computational resources using FL frameworks~\citep{Flower,FedScale,FedML}.

While the field has enjoyed abundant scientific and industry attention, the privacy and communication benefit it provides cause significant challenges in efficiently scaling and evolving federated systems. Crucially, training a single global model is unsuitable when unusual clients require partial or complete personalisation of the model to their local data distribution.

In its standard form, FL operates directly on clients using a centralised server to distribute model parameters and then aggregate them after client training; this process is repeated for multiple rounds. However, data in FL is subject to attributes such as client geographic location, sensor hardware, and behaviour. Due to these factors, the federated distribution violates the Independent and Identically Distributed (IID) assumption. Such \emph{data heterogeneity}~\citep[sec. 3.1]{AdvancedAndOpenProblems} is interwoven with \emph{systems heterogeneity}~\citep[sec. 7.2]{AdvancedAndOpenProblems} since clients have different computational abilities and network speeds. Additionally, the communication costs of transmitting model parameters between servers and clients are non-trivial.
% Since data heterogeneity makes obtaining a single global model efficient on all client data distributions unfeasible, we are concerned with creating arbitrary levels of personalisation in the form of Hierarchical Federated Learning in a manner that improves learning efficiency and allows such systems to evolve.

Efficiency and scalability have been at the centre of FL research since \citet{GoogleKeyboard} applied FL to mobile keyboard prediction at Google. Building on top of \citet{GoogleKeyboard}, \citet{ScaleSystemDesign} showed that FL could be used to train models over tens of millions of smartphones. However, despite the optimistic billion-device forecasts of \citet{ScaleSystemDesign}, several limitations to the efficiency of FL emerged. These limitations are threefold: (a) synchronous FL can only effectively use hundreds of devices every round, (b) federated training is considerably slower than centralised training, (c) user devices are unreliable, leading to dropout and stragglers. These limitations received further attention in the empirical evaluation of \citet{LargeCohorts}.

\citet{LargeCohorts} show that the performance of FL does not scale as expected when the number of clients trained every round increases despite previous theoretical work~\citep{TighterTheory} indicating the contrary. Their experimental results show that the primary limitation of increasing cohort size under Non-IID settings is the miss-alignment of client models, indicated by a near-zero cosine similarity between updates. This miss-alignment limits the impact of a round, causes diminishing returns to increasing cohort size, and results in an inability to learn efficiently from client data in parallel. Thus, given that FL is highly parallel, its scalability is limited by the ability to learn from clients on a per-sample basis. Asynchronous Federated Learning systems~\citep{AsynchronousFLonHetDevicesSurvey,FedBuff,AsyncrhonousOnlineFL,AsyncDropout}, such as Meta's PAPAYA~\citep{PAPAYA}, show promise in improving efficiency and scalability; however, they introduce the new issues of staleness and high update variance.

Evolving FL systems is also a major challenge. The datasets of clients forming a federated network are generally not static. Clients may delete data immediately after generation, periodically, or ad-hoc based on memory needs or owner requests. Furthermore, the characteristics of newly added data can change gradually or immediately. For example, seasonal transitions shift captured images slowly, while changing locations leads to discrete changes. This problem is known as dataset shift~\citep[sec. 3.1]{AdvancedAndOpenProblems} and represents \emph{intra-client} heterogeneity rather than the common \emph{inter-client} heterogeneity. Even works which maintain persistent local models~\citep{Ditto,FlWithPersonalisationLayers,AdaptivePersonalisedFederatedLearning,FederatedLearningMixtureOfGlobalAndLocal} assume that this model is only used within FL rounds, obfuscating such shifts.\\

\noindent \textbf{Structure:} Given the challenges of FL and shortcomings of previous work, detailed in \cref{sec:back}, the proposed research for my PhD aims to \emph{provide flexible personalisation for highly efficient and scalable FL systems} by exploring the research questions outlined in \cref{sec:proposal:research_questions}. To achieve this goal, \cref{sec:proposal} propose a new family of FL algorithms called Bidirectional Hierarchical Federated Learning (B-HFL) based on \cref{alg:B-HFL}. Building on these foundations, I outline the research directions of my PhD that B-HFL enables in \cref{sec:proposal:research_directions}. Finally, I summarise already completed work \cref{sec:completed_work} and provide a detailed timeline for the PhD in \cref{sec:timeplan}.

\noindent \textbf{Notice:} The following subsection provides a \emph{summary} of B-HFL and the contributions it enables.
\section{Summary of Proposed Research}
Bidirectional Hierarchical FL (B-HFL) will address the aforementioned personalisation, efficiency, and evolution challenges by constructing hierarchical federated network structures that allow bidirectional and potentially cyclical dataflow where each leaf is a client and each internal node is a server. As a result, levels in the tree closer to the leaves are more personalised to the specific client population of a subtree, and those closer to the root provide more generalisable models. Furthermore, since B-HFL treats nodes homogeneously, every intermediary node can operate independently like a standard FL server, using synchronous or asynchronous execution of its sub-nodes depending on constraints.

This proposal builds upon the work done by \citet{EuroMLSysWorkshop} and \citet{OperaWorkshop} on personalised and hierarchical FL. The proposed system communicates data in as shown in \cref{alg:B-HFL} and \cref{fig:TreeStructure}. Crucially, model parameters can flow bidirectionally, and nodes can apply partial updates from their parents via aggregation. Furthermore, each node can weight children and parent parameters differently while using methods such as the adaptive server optimisers~\citep{FedOPT}, model-interpolation~\citep{AdaptivePersonalisedFederatedLearning,FederatedLearningMixtureOfGlobalAndLocal}, or training-based methods~\citep{Ditto,EWC,DeepMutualLearning,PersonalisedFLFirstOrder}. Adaptive algorithms and model interpolation are particularly relevant as they allow each node in the tree to distinguish itself based on its previous state without necessitating additional parameter tuning. Furthermore, since each edge-server controls fewer clients, the diminishing effects of increasing cohort sizes are avoided.  Finally, in the case where client cohorts are meaningfully clustered, this structure may allow a drastic increase in the sample efficiency of the system as each cluster decides how to optimise the generalisation-personalisation trade-off~\citep{PersonalisationGeneralisationTradeoff,Auxo}. The potential contributions to the field of Federated Learning include:
\begin{enumerate}
    \item A family of efficient algorithms with minute control of personalisation and generalisation, capable of achieving communication efficiency in hierarchical networks.
    \item The investigation of three techniques enabled by B-HFL\@: (a) allowing leaf nodes to maintain persistent local models training asynchronously to tackle dataset shift, (b) making any node in the tree capable of training with a proxy dataset to inject general information, (c) constructing  vertical connections in the tree, similar to residual connections~\citep{ResNet}, to allow customisable dataflow without changing the underlying communication infrastructure.
    \item Extensive empirical evaluations considering scenarios with or without meaningful client clusters in language and image/speech recognition tasks leading to intended publication at \href{https://iclr.cc/}{ICLR} or \href{https://mlsys.org/}{MLSys}. This publication will be followed up by a work intended for \href{https://sigmobile.org/mobicom/2023/}{MobiCom} investigating asynchronous training on resource-constrained devices with dataset shift using the Raspberry Pi FL cluster at Cambridge ML Systems.
\end{enumerate}









