Federated Learning (FL) is a distributed Machine Learning (ML) paradigm allowing multiple clients to train a shared collaborative model without communicating private data. It was introduced by \citet{FedAvg} as a means of reducing communication costs and lessening the privacy concerns of storing sensitive data in a centralised location, following the principle of data minimisation outlined in the \citet{White_House_Report} privacy report. These properties have led to FL applications with large cohorts of small edge devices, e.g., mobile keyboard prediction~\citep{GoogleKeyboard} for Android phones, and settings with larger entities subject to privacy requirements, e.g., hospitals~\citep{FLmedicine}. \citet{AdvancedAndOpenProblems} distinguish them as cross-device and cross-silo FL\@.

The growth in the preponderance of Federated Learning since the publication of \citet{FedAvg} can be ascribed to two primary trends. First, an increase in the privacy requirements of consumers and legal frameworks has put pressure on technology companies. This pressure drove interest in privacy-preserving ML at major corporations such as Google~\citep{FedAvg,GoogleKeyboard,tensorflowfederated,PracticalPrivateFLkairouz21b}, Microsoft~\citep{FLINT,Flute}, Meta~\citep{PAPAYA,FedBuff}, and Apple~\citep{AppleFL}. Second, ML has extended to domains with strict privacy requirements such as healthcare~\citep{FLmedicine,FutureOfHealth,BigDataCancer}, Human Activity Recognition~(HAR)~\citep{HARusingFL_2018,ClusterFL} or collaborations between competing corporations~\citep{SustainableIncentive,TowardsFairPrivacyPreservingFL}. Moreover, the emergence of Large Language Models (LLMs)~\citep{OpportunitiesAndRisksLLM} has made accessing private language corpora advantageous, leading to the development of Federated Natural Language Processing (FedNLP)~\citep{FedNLP}. Similarly, the release of openly available LLM pre-trained weights~\citep{LLaMA} allows collaboration between entities with low computational resources using FL frameworks~\citep{Flower,FedScale,FedML}.

In its standard form, FL operates directly on clients using a centralised server to distribute model parameters and then aggregate them after client training; this process is repeated for multiple rounds. However, data in FL is subject to attributes such as client geographic location, sensor hardware, and behaviour. Due to these factors, the federated distribution violates the Independent and Identically Distributed (IID) assumption. Such \emph{data heterogeneity}~\citep[sec. 3.1]{AdvancedAndOpenProblems} is interwoven with \emph{systems heterogeneity}~\citep[sec. 7.2]{AdvancedAndOpenProblems} since clients have different computational abilities and network speeds. The communication costs of transmitting model parameters between servers and clients are non-trivial. Furthermore, training a single global model is unsuitable when unusual clients require partial or complete personalisation of the model to their local data distribution.

Efficiency and scalability have been at the centre of FL research since \citet{GoogleKeyboard} applied FL to mobile keyboard prediction at Google. Building on top of \citet{GoogleKeyboard}, \citet{ScaleSystemDesign} showed that FL could be used to train models over tens of millions of smartphones. However, despite the optimistic billion-device forecasts of \citet{ScaleSystemDesign}, several limitations to the efficiency of FL emerged. These limitations are threefold: (a) synchronous FL can only effectively use hundreds of devices every round, (b) federated training is considerably slower than centralised training, (c) user devices are unreliable, leading to dropout and stragglers. These limitations received further attention in the empirical evaluation of \citet{LargeCohorts}.

\citet{LargeCohorts} show that the performance of FL does not scale as expected when the number of clients trained every round increases despite previous theoretical work~\citep{TighterTheory} indicating the contrary. Their experimental results show that the primary limitation of increasing cohort size under Non-IID settings is the miss-alignment of client models, indicated by a near-zero cosine similarity between updates. This miss-alignment limits the impact of a round, causes diminishing returns to increasing cohort size, and results in an inability to learn efficiently from client data in parallel. Thus, its scalability is limited by the ability to learn from clients on a per-sample basis. Asynchronous Federated Learning systems~\citep{AsynchronousFLonHetDevicesSurvey,FedBuff,AsyncrhonousOnlineFL,AsyncDropout}, such as Meta's PAPAYA~\citep{PAPAYA}, show promise in improving efficiency and scalability; however, they introduce the new issues of staleness and high update variance.

While the field has enjoyed abundant scientific and industry attention, the privacy and communication benefits it provides cause significant challenges in efficiently scaling and evolving federated systems. It is unclear how FL systems should behave in three related scenarios. First, clients in the system may contain their data that must be kept on-premise and entirely private but also control other clients with the same constraints. Such scenarios occur frequently in healthcare, where patient data may only be moved under stringent regulatory conditions. For example, local hospitals may want to collaborate with regional hospitals, which may want to collaborate at a national or international level. Second, organisations such as user groups, corporations, or governments may create independent federated systems with different security and privacy expectations and then desire to collaborate. In such cases, it is unclear how the governance of the new federation can be set up without extensive legal arbitrage. Even if privacy and security expectations can be aligned, the difficulties merging several federations of clients under one central server are considerable from a geographic and technical perspective as the entire infrastructure of the involved systems needs to be re-routed. Third, simple server-client federations can enter the previous two categories as new clients join or expectations and requirements change. For example, in cross-silo FL, an organisation client may choose to create its federated system for devices in controls.


\noindent \textbf{Structure} Given the challenges of FL and shortcomings of previous work, detailed in \cref{sec:back}, the proposed research for my PhD aims to address these three scenarios through Hierarchical Federated Learning by exploring the research questions outlined in \cref{sec:proposal:research_questions}. To achieve this goal, \cref{sec:proposal} propose a new family of FL algorithms called Bidirectional Hierarchical Federated Learning (B-HFL) based on \cref{alg:B-HFL}. Building on these foundations, I outline the research directions of my PhD that B-HFL enables in \cref{sec:proposal:research_directions}. Finally, I summarise previously completed work in \cref{sec:completed_work} and present a detailed timeline for the PhD in \cref{sec:timeplan}.

\noindent \textbf{Notice} The following subsection provides a \emph{summary} of B-HFL and the contributions it enables.
\section{Summary of Proposed Research}
Bidirectional Hierarchical FL (B-HFL) would address the aforementioned personalisation, efficiency, and evolution challenges by constructing hierarchical federated network structures that allow bidirectional and potentially cyclical dataflow where each leaf is a client and each internal node is a server. As a result, levels in the tree closer to the leaves are more personalised to the specific client population of a subtree, while those closer to the root provide more generalisable models. Furthermore, since B-HFL treats nodes homogeneously, every intermediary node can operate independently like a standard FL server, using synchronous or asynchronous execution of its sub-nodes depending on constraints.

The proposed research builds upon the work done by \citet{EuroMLSysWorkshop} and \citet{OperaWorkshop} on personalised and hierarchical FL. The proposed system communicates data as shown in \cref{alg:B-HFL} and \cref{fig:TreeStructure}. Crucially, model parameters can flow bidirectionally, and nodes can apply partial updates from their parents via aggregation. Furthermore, each node can weigh children and parent parameters differently while using methods such as the adaptive server optimisers~\citep{FedOPT}, Iterative Moving Averaging (IMA)~\citep{UnderstandingModelAveragingInFL} of model parameters, model-interpolation~\citep{AdaptivePersonalisedFederatedLearning,FederatedLearningMixtureOfGlobalAndLocal}, or training-based methods~\citep{Ditto,EWC,DeepMutualLearning,PersonalisedFLFirstOrder}. Adaptive algorithms and IMA are particularly relevant as they allow each node in the tree to distinguish itself based on its previous state without necessitating additional parameter tuning. Furthermore, since each edge server controls few clients, the diminishing effects of increasing cohort sizes are avoided.  Finally, when cohorts are meaningfully clustered, this structure may allow an increase in the sample efficiency of the system as each cluster decides how to optimise the generalisation-personalisation trade-off~\citep{PersonalisationGeneralisationTradeoff,Auxo}. The potential contributions to the field of Federated Learning include:
\begin{enumerate}
    \item A family of efficient algorithms with minute control of personalisation and generalisation, capable of achieving communication efficiency in hierarchical networks.
    \item The investigation of three techniques enabled by B-HFL\@: (a) allowing leaf nodes to maintain persistent local models training asynchronously to tackle dataset shift, (b) making any node in the tree capable of training with a proxy dataset to inject general information, (c) constructing vertical connections in the tree, similar to residual connections~\citep{ResNet}, to allow customisable dataflow without changing the underlying communication infrastructure.
    \item Extensive empirical evaluations considering scenarios with or without meaningful client clusters in language and image/speech recognition tasks leading to intended publication at \href{https://iclr.cc/}{ICLR} or \href{https://mlsys.org/}{MLSys}. This publication will be followed up by a work intended for \href{https://sigmobile.org/mobicom/2023/}{MobiCom} investigating asynchronous training on resource-constrained devices with dataset shift using the Raspberry Pi FL cluster at Cambridge ML Systems.
\end{enumerate}









