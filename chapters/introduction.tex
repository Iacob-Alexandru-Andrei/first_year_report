
Federated Learning (FL) is a distributed Machine Learning (ML) paradigm allowing multiple clients to train a shared collaborative model without communicating private data. It was introduced by \citet{FedAvg} as a means of reducing communication costs and lessening the privacy concerns of storing sensitive data in a centralised location, following the principles of focused collection and data minimisation outlined in the \citet{White_House_Report} privacy report. These properties have led to FL applications with large cohorts of small edge devices, such as mobile keyboard prediction~\citep{GoogleKeyboard} for Android phones, and settings with larger entities subject to privacy requirements, such as hospitals~\citep{FLmedicine}. These two settings are distinguished by \citet{AdvancedAndOpenProblems} as cross-device and cross-silo FL\@.

The growth in the preponderance of Federated Learning since the publication of \citet{FedAvg} can be ascribed to two primary trends. First, an increase in the privacy requirements of consumers and legal frameworks has put pressure on technology companies. This pressure drove interest in privacy-preserving ML at major corporations such as Google~\citep{FedAvg,GoogleKeyboard,tensorflowfederated,PracticalPrivateFLkairouz21b}, Microsoft~\citep{FLINT,Flute}, Meta~\citep{PAPAYA,FedBuff}, and Apple~\citep{AppleFL}. Second, ML has extended to domains with strict privacy requirements such as healthcare~\citep{FLmedicine,FutureOfHealth,BigDataCancer}, Human Activity Recognition~(HAR)~\citep{HARusingFL_2018,ClusterFL} or collaborations between competing corporations~\citep{SustainableIncentive,TowardsFairPrivacyPreservingFL}. Moreover, the emergence of Large Language Models (LLMs)~\citep{OpportunitiesAndRisksLLM} has made accessing private language corpora advantageous, leading to the development of Federated Natural Language Processing (FNLP)~\citep{FedNLP}. Similarly, the release of openly available LLM pre-trained weights~\citep{LLaMA} allows collaboration between entities with low computational resources using FL frameworks~\citep{Flower,FedScale,FedML}.

While the field has enjoyed abundant scientific and industry attention, the privacy and communication benefit it provides cause significant challenges in efficiently scaling and evolving federated systems. Crucially, the compromise of training a single global model is unsuitable when unusual clients require partial or complete personalisation of the model to their local data distribution. 

This goal of this proposal is to \emph{provide arbitrary degrees of personalisation in highly scalable FL systems.}

\subsection{Challenges}
In its standard form, FL operates directly on clients using a centralised server to distribute model parameters and then aggregate them after client training; this process is repeated for multiple rounds. However, data in FL is subject to attributes such as client geographic location, sensor hardware, and behaviour. Due to these factors, the federated distribution violates the Independent and Identically Distributed (IID) assumption. Such \emph{data heterogeneity}~\citep[sec. 3.1]{AdvancedAndOpenProblems} is interwoven with \emph{systems heterogeneity}~\citep[sec. 7.2]{AdvancedAndOpenProblems} since clients have different computational abilities and network speeds. Additionally, the communication costs of transmitting model parameters between servers and clients are non-trivial. Since data heterogeneity makes obtaining a single global model efficient on all client data distributions unfeasible, we are concerned with creating arbitrary levels of personalisation in the form of Hierarchical Federated Learning in a manner that improves learning efficiency and allows such systems to evolve.

\todo{Add more on Personalisation}

Efficiency and scalability have been at the centre of FL research since \citet{GoogleKeyboard} applied FL to mobile keyboard prediction at Google. Building on top of \citet{GoogleKeyboard}, \citet{ScaleSystemDesign} showed that FL could be used to train models over tens of millions of smartphones. However, despite the optimistic billion-device forecasts of \citet{ScaleSystemDesign}, several limitations to the efficiency of FL emerged. These limitations are threefold: (a) synchronous FL can only effectively use hundreds of devices every round, (b) federated training is considerably slower than centralised training, (c) user devices are unreliable, leading to dropout and stragglers. These limitations received further attention in the empirical evaluation of \citet{LargeCohorts}.

\citet{LargeCohorts} show that the performance of FL does not scale as expected when the number of clients trained every round increases despite previous theoretical work~\citep{TighterTheory} indicating the contrary. Their experimental results show that the primary limitation of increasing cohort size under Non-IID settings is the miss-alignment of client models, indicated by a near-zero cosine similarity between updates. This miss-alignment limits the impact of each round, causes diminishing returns to increasing cohort size, and results in an inability to learn efficiently from client data in parallel. Thus, given that FL algorithms are highly parallel, scalability in FL is strongly limited by the ability to learn from clients on a per-sample basis. Furthermore, while the original investigations of \citet{ScaleSystemDesign,LargeCohorts} were cross-device, the problem of efficiently learning from clients also applies to cross-silo settings.

The datasets of clients forming a federated network are generally not static. Clients may delete data immediately after generation, periodically, or ad-hoc based on memory needs or owner requests. Furthermore, the characteristics of newly added data can shift over time in either a gradual or immediate manner. For example, in Image Recognition tasks, seasonal transitions can shift captured images slowly, while changing locations or upgrading the camera hardware may lead to discrete changes. This problem is known as dataset shift~\citep[sec. 3.1]{AdvancedAndOpenProblems} and represents \emph{in-client} heterogeneity rather than the more common \emph{cross-client} heterogeneity. Synchronous Federated Learning algorithms~\citep{FedAvg,FedOPT,FedMA,QFedAvg,TERM} assume that the clients only operate on the federated model received at the start of a round. Even works which maintain persistent local models, such as (Ditto)~\citep{Ditto}, assume that this persistent model is only used within FL rounds. Thus, current approaches cannot capture changes in the data distribution of a client. Asynchronous Federated Learning systems~\citep{AsynchronousFLonHetDevicesSurvey,FedBuff,AsyncrhonousOnlineFL}, such as Meta's PAPAYA~\citep{PAPAYA}, do allow clients to train outside round boundaries. However, they similarly assume that clients only train on the latest copy of the federated model they can access when they pull from the server.

\subsection{Proposal Summary}

Adressing the aforementiond challenges of personalsiation, efficiency and evolution is achieved by constructing hierarchical tree-like federated network structures that allow bidirectional and potentially cyclical dataflow where each leaf is a client, and each internal node is a server capable of training on proxy public data. As a result, levels in the tree closer to the leaves are more personalised to the specific client population of a subtree, and those closer to the root provide more generalisable models. We call this approach Bidirectional Hierarchical Federated Learning (B-HFL). Furthermore, we allow leaf clients in these structures to execute asynchronous training using persistent models to account for temporal shifts in their data distributions to facilitate evolution.

This proposal builds upon the work done by \citet{EuroMLSysWorkshop} and \citet{OperaWorkshop} on personalised and hierarchical FL. The proposed system communicates data in as shown in \cref{alg:B-HFL} and \cref{fig:TreeStructure}. Crucially, model parameters can flow bidirectionally, and nodes can apply partial updates from their parents via aggregation. Furthermore, each node can weight children and parent parameters differently while using methods such as the adaptive server optimisers~\citep{FedOPT} or training-based methods~\citep{Ditto,EWC,DeepMutualLearning,PersonalisedFLFirstOrder}. Adaptive algorithms are particularly relevant as they allow each node in the tree to distinguish itself based on its previous state without necessitating additional parameter tuning. Finally, in the case where client cohorts are meaningfully clustered, this structure may allow a drastic increase in the sample efficiency of the system as each cluster decides how to optimise the generalisation-personalisation trade-off~\citep{PersonalisationGeneralisationTradeoff}. The potential contributions to the field include:
\begin{enumerate}
    \item A family of efficient and scalable hierarchical FL algorithms allowing fine-grained control over personalisation and generalisation from the global root to fully-personalised leaves.
    \item The investigation of three complimentary techniques enabled by such hierarchical structures: (a) allowing leaf clients to maintain persistent local models training asynchronously to tackle dataset shift, (b) making any node in the tree capable of training with a proxy dataset to inject more general information, (c) constructing additional vertical connections in the tree similar to residual connections~\citep{ResNet} to allow highly customisable dataflow without changing the underlying communication infrastructure.
    \item Extensive empirical evaluations considering scenarios with or without meaningful client clusters in language and speech recognition tasks leading to publications at \href{https://iclr.cc/}{ICLR} or \href{https://mlsys.org/}{MLSys}. This publication will be followed up by a work intended for \href{https://sigmobile.org/mobicom/2023/}{MobiCom} investigating asynchronous training on resource-constrained devices with dataset shift using the Raspberry Pi FL cluster at Cambridge ML Systems.
\end{enumerate}









