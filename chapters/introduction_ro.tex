Învățarea Federată (referită ca FL din termenul englez ``Federated Learning'') este o paradigma de Învățare Automată (referită ca ML din termenul englez ``Machine Learning'') distribuită care permite mai multor clienți să instruiască un model colaborativ comun fără a comunica date private. Aceasta a fost introdusa de \citet{FedAvg} ca un mijloc de reducere a costurilor de comunicare și de diminuare a problemelor de confidențialitate legate de stocarea datelor sensibile într-o locație centralizată, urmând principiile colectării concentrate și a minimizării datelor descrise în raportul de confidențialitate \citet{White_House_Report}. Aceste proprietăți au condus la aplicații FL cu cohorte mari de dispozitive de mici, cum ar fi predicția tastaturii mobile~\citep{GoogleKeyboard} pentru telefoanele Android, și aplicații cu entități mai mari supuse cerințelor de confidențialitate, cum ar fi spitalele~\citep{FLmedicine}. Aceste două tipuri de Învățarea Federată sunt distinse de \citet{AdvancedAndOpenProblems} ca FL cross-device și cross-silo.

Creșterea preponderenței FL de la publicarea \citet{FedAvg} poate fi atribuită către două trenduri. În primul rând, o creștere a cerințelor de confidențialitate ale consumatorilor și a cadrului juridic a pus presiune pe companiile de tehnologie. Această presiune a condus la interesul pentru ML care protejează confidențialitatea în cadrul corporațiilor majore precum Google~\citep{FedAvg,GoogleKeyboard,ScaleSystemDesign}, Microsoft~\citep{FLINT}, Meta~\citep{PAPAYA,FedBuff}, și Apple~\citep{AppleFL}. În al doilea rând, ML s-a extins către domenii cu cerințe stricte de confidențialitate cum ar fi sănătatea~\citep{FLmedicine}, Recunoașterea Activităților Umane~\citep{HARusingFL_2018,ClusterFL} sau colaborările între corporații concurente~\citep{SustainableIncentive}. Mai mult, apariția Modelelor de Limbaj Mari (referită ca LLM din termenul englez ``Large Language Model'')~\citep{OpportunitiesAndRisksLLM} a făcut accesarea colecțiilor private de limbaj natural avantajoasă, conducând la dezvoltarea Învățării Federate de Procesare a Limbajului Natural~\citep{FedNLP}. În mod similar, lansarea de ponderi~(weights) pre-antrenate open source~\citep{LLaMA} permite colaborarea între entități cu resurse computaționale reduse utilizând framework-uri de FL~\citep{Flower,FedScale,FedML}.

Deși domeniul s-a bucurat de o atenție științifică și industrială sporită, beneficiile pe care le oferă în ceea ce privește confidențialitatea și comunicarea cauzează provocări semnificative în ceea ce privește creșterea eficienței și evoluția sistemelor federate. În mod crucial, compromisul de a antrena un singur model global nu este potrivit atunci când clienții eterogeni necesită personalizare parțială sau completă a modelului pentru distribuția lor locală de date.

Această lucrare propune abordarea provocărilor menționate prin construirea de structuri de rețea federate ierarhice de tip arbore, care permit flux de date bidirecțional și potențial ciclic, unde fiecare frunză este un client, iar fiecare nod intern este un server capabil să se instruiască pe date publice proxy. În consecință, nivelurile din arbore mai apropiate de frunze sunt mai personalizate pentru populația specifică de clienți a unui subarbore, iar cele mai apropiate de rădăcină oferă modele mai generalizabile. Această abordare este denumită Învățare Federată Ierarhică Bidirecțională (referită ca B-HFL din termenul englez ``Bidirectional Hierarchical Federated Learning''). Mai mult, clienților frunză din aceste structuri le este permis să execute antrenare asincronă folosind modele persistente pentru a ține cont de schimbările temporale în distribuțiile lor de date și a le facilita evoluția.
\subsection{Motivație}

În forma sa standard, FL operează direct pe clienți folosind un server centralizat pentru a distribui parametrii modelului și apoi pentru a-i agrega după antrenarea clientului; acest proces este repetat pentru mai multe runde. Cu toate acestea, datele în FL sunt supuse atributelor precum locația geografică a clientului, specificațiile senzorului și comportamentul clientului. Datorită acestor factori, distribuția federată încalcă ipoteza Independentei și Identității Distribuție (IID). O astfel de \emph{eterogenitate a datelor}~\citep[sec. 3.1]{AdvancedAndOpenProblems} este împletită cu \emph{eterogenitatea sistemelor}~\citep[sec. 7.2]{AdvancedAndOpenProblems} deoarece clienții au abilități de calcul și viteze diferite de rețea. În plus, costurile de comunicare ale transmiterii parametrilor modelului între servere și clienți sunt semnificative. Deoarece eterogenitatea datelor face ca obținerea unui singur model global eficient pe toate distribuțiile de date ale clienților să fie imposibilă, suntem preocupați de crearea unor niveluri arbitrare de personalizare sub forma Învățării Federate Ierarhice într-un mod care îmbunătățește eficiența învățării și permite acestor sisteme să evolueze.
\subsubsection{Eficiență}

Eficiența și scalabilitatea au fost în centrul cercetării FL de la momentul în care \citet{GoogleKeyboard} a aplicat FL la predicția tastaturii mobile la Google. Pe baza lucrării \citet{GoogleKeyboard}, \citet{ScaleSystemDesign} s-a demonstrat că FL poate fi folosit pentru a instrui modele peste zeci de milioane de smartphone-uri. Cu toate acestea, în ciuda prognozelor optimiste de un miliard de dispozitive ale \citet{ScaleSystemDesign}, au apărut multiple limitări ale eficienței FL. Aceste limitări sunt de trei feluri: (a) FL sincron poate folosi eficient doar sute de dispozitive în fiecare rundă, (b) instruirea federată este considerabil mai lentă decât instruirea centralizată, (c) dispozitivele utilizatorilor sunt nesigure, ceea ce duce la deconectarea acestora. Aceste limitări au primit o atenție suplimentară în evaluarea empirică a \citet{LargeCohorts}.

\citet{LargeCohorts} arată că performanța FL nu se îmbunătățește așa cum era de așteptat atunci când numărul de clienți instruiți în fiecare rundă crește, în ciuda lucrărilor teoretice anterioare~\citep{TighterTheory} care indicau contrariul. Rezultatele lor experimentale arată că principala limitare a creșterii dimensiunii cohortelor în setări Non-IID este diferența dintre actualizările de model ale clienților, indicată printr-un cosinus aproape zero între acestea. Această diferență limitează impactul fiecărei runde, provoacă randamente diminuate la creșterea dimensiunii cohortelor și rezultă în incapacitatea de a învăța eficient din datele clientului în paralel. Astfel, având în vedere că algoritmii FL sunt intrinsec paraleli, scalabilitatea în FL este limitată de capacitatea de a învăța eficient pe baza fiecărui exemplu de antrenament al clienților. În plus, în timp ce investigațiile originale ale \citet{ScaleSystemDesign,LargeCohorts} erau cross-device, problema învățării eficiente de la clienți se aplică și setărilor cross-silo.


\subsubsection{Evoluție}
Seturile de date ale clienților care formează o rețea federată nu sunt în general statice. Clienții pot șterge datele imediat după generare, periodic sau ad-hoc, în funcție de necesitățile de memorie sau cererile proprietarului. În plus, caracteristicile datelor nou adăugate se pot modifica în timp într-un mod gradat sau imediat. De exemplu, în sarcinile de recunoaștere a imaginilor, tranzițiile sezoniere pot modifica încet imaginile capturate, în timp ce schimbarea locațiilor sau actualizarea hardware-ului camerei poate duce la schimbări discrete. Această problemă este cunoscută sub numele de ``shift'' al setului de date~\citep[sec. 3.1]{AdvancedAndOpenProblems} și reprezintă eterogenitatea \emph{în-client} mai degrabă decât eterogenitatea \emph{între-clienți} mai comună. Algoritmii sincroni de Învățare Federată~\citep{FedAvg,FedOPT,QFedAvg} presupun că antrenarea clienților se realizează doar pe modelul federat primit la începutul unei runde. Chiar și sistemele care mențin modele locale persistente, cum ar fi (Ditto)~\citep{Ditto}, presupun că acest model persistent este folosit doar în timpul rundelor FL. Prin urmare, abordările actuale nu pot capta schimbările în distribuția datelor unui client. Sistemele asincrone de FL~\citep{AsynchronousFLonHetDevicesSurvey,FedBuff}, cum ar fi PAPAYA de la Meta~\citep{PAPAYA}, permit clienților să fie utilizați în afara limitelor rundei. Cu toate acestea, ele presupun în mod similar antrenarea clienților doar pe cea mai recentă copie a modelului federat pe care o pot accesa.

\subsection{Rezumatul Propunerii}

Această propunere extinde lucrările realizate de \citet{EuroMLSysWorkshop} și \citet{OperaWorkshop} pe subiectele de Învățare Federată personalizată, respectiv ierarhică. Sistemul propus comunică datele într-o structură de tip arbore, așa cum este ilustrat în \cref{fig:TreeStructure}. În mod crucial, parametrii modelului pot circula în ambele sensuri, iar nodurile pot aplica actualizări parțiale de la părinții lor prin agregare. În plus, fiecare nod poate asocia o pondere diferită parametrilor copiilor și părinților în timp ce folosește metode precum optimizatori adaptivi de server~\citep{FedOPT} sau metodele bazate pe antrenare~\citep{Ditto,EWC,DeepMutualLearning}. Algoritmii adaptivi sunt relevanți deoarece permit fiecărui nod din arbore să se distingă în funcție de starea sa anterioară fără a necesita ajustarea suplimentară a parametrilor. În final, în cazul în care cohortele de clienți sunt grupate în mod semantic, această structură poate permite o creștere drastică a eficienței sistemului, deoarece fiecare cluster decide cum să optimizeze compromisul generalizare-personalizare~\citep{PersonalisationGeneralisationTradeoff}. Contribuțiile potențiale ale propunerii către acest domeniu includ:
\begin{enumerate}
    \item O familie de algoritmi FL ierarhici și scalabili care permit un control fin asupra personalizării și generalizării de la rădăcina globală până la frunzele complet personalizate.
    \item Investigarea a trei tehnici complementare permise de aceste structuri ierarhice: (a) permiterea clienților de la nivelul frunzelor să mențină modele locale persistente care se antrenează asincron pentru a aborda shiftul setului de date, (b) făcând ca orice nod din arbore să fie capabil să se antreneze cu un set de date proxy pentru a injecta o perspectivă generală modelului, (c) construirea de conexiuni verticale suplimentare în arbore similare cu conexiunile reziduale~\citep{ResNet} pentru a permite un flux de date modificabil fără a schimba infrastructura de comunicare de bază.
    \item Evaluări empirice extinse care iau în considerare scenarii cu sau fără clustere semnificative de clienți în sarcini de recunoaștere a limbajului menite publicării la conferințele \href{https://iclr.cc/}{ICLR} sau \href{https://mlsys.org/}{MLSys}. Această publicație va fi urmată de o lucrare destinată pentru \href{https://sigmobile.org/mobicom/2023/}{MobiCom} care investighează antrenamentul asincron pe dispozitive cu resurse limitate cu shift de set de date folosind clusterul Raspberry Pi FL din laboratorul Cambridge ML Systems.
\end{enumerate}
