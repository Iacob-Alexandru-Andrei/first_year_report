% \listoftodos[Background]
\todo{Scalability experiments with increasing cohort sizes}

Obiectivul standard FL poate fi modelat așa cum se vede în \cref{eq:flObjective}
\begin{equation} \label{eq:flObjective}
    \underset{\theta}{\min} F(\theta) = \sum_{c \in C} p_c F_c(\theta) \ ,
\end{equation}
unde \(F\) este obiectivul federat, $C$ este setul de clienți, $\theta$ este modelul și \(F_c\) este funcția de cost a clientului \(c\) ponderată de fracția lui $p_c$ din numărul total de exemple. Această formulare presupune că este antrenat un singur model global fără a ține cont de distribuția performanței sale pe seturile de date ale clienților. Federated Averaging (FedAvg)~\citep{FedAvg} antrenează modelul global pe clienți, pentru fiecare rundă $t$ sumează actualizarea \(\theta_t^c - \theta_t\) de la clientul $c$ ponderată de \(p_c\) cu modelul rundei anterioare \(\theta_t\) folosind rata de învățare \( \eta \),  prezentat în \cref{eq:FedAvg}
\begin{equation} \label{eq:FedAvg}
    \theta_{t+1} = \theta_t + \eta \left( \sum_{c \in C} p_c \left(\theta_t^c - \theta_t \right) \ \right) \ .
\end{equation}

Incapacitatea de a aduna informațiile clienților în același rezervor de date și necesitatea de a construi combinații aproximative de parametri ai modelului, reprezintă principalele cauze ale provocărilor din FL.

\subsection{Eterogenitate}\label{background:data_heterogeneity}
S-a demonstrat că datele Non-IID au un impact atât asupra performanței practice \citep{FLwithNonIID}, cât și asupra limitelor teoretice de convergență \citep{OnTheConvergenceOfFedAvgOnNonIIDdata}. Prin urmare, merită detaliate unele forme de eterogenitate pe care \citet{AdvancedAndOpenProblems} le identifică. Cea mai frecvent abordată formă este quantity skew cauzată de faptul că clienții au cantități diferite de date disponibile. Algoritmii standard de FL tratează eficient quantity skew prin intermediul unei reponderări simple (\cref{eq:FedAvg}). Celălalt tip de eterogenitate frecvent considerat este asimetria distribuției etichetelor. Deși aceste forme de eterogenitate au fost cele mai investigate, situațiile în care caracteristicile și etichetele nu sunt relaționate în același mod între clienți sunt mai nefavorabile și pot necesita o formă de clustering sau personalizare pentru a  fi abordate. În cel mai rău caz, fiecare client poate reprezenta un task complet diferit, similar cu Învățarea Automată Multi-task (referită ca MTL din termenul englez ``Multi-task Learning''), cu posibil nicio suprapunere în spațiul de soluții al clienților.

\paragraph{Eterogenitatea sistemului (hardware)} Dispozitivele din cadrul rețelei federate pot diferi în ceea ce privește capacitatea de calcul, stocarea, viteza rețelei și fiabilitatea. Ele se pot diferenția, de asemenea, de ele însele într-un alt moment de timp în funcție de puterea baterie sau conexiunea la rețea. Este important de menționat că variațiile în hardware-ul care generează date, cum ar fi senzorii, sunt legate de eterogenitatea lor. Cu toate acestea, eterogenitatea sistemului afectează procesul FL independent de date. De exemplu, hardware-ul mai lent poate duce la clienți care întârzie, care prelungesc rundele în FL sincron sau operează pe parametri învechiți în FL asincron. În plus, nesiguranța rețelei creează abandon, ceea ce necesită supraeșantionarea clienților~\cite{ScaleSystemDesign} și afectează eficacitatea menținerii stării clienților între runde.

\paragraph{Shiftul temporal al setului de date} Antrenarea eficientă a modelelor ML pe parcursul întregii vieți este obiectivul învățării continue~\citep{ContinualLearningSurvey}. Cu toate acestea, aplicarea învățării continue în contextul FL este problematică din două motive principale. În primul rând, obiectivul de optimizare (\cref{eq:flObjective}) intenționează să găsească un model de compromis pentru toți clienții și nu se poate potrivi exact cu toate datele lor. Prin urmare, dacă setul de date al unui client se schimbă independent de întreaga rețea, modelul federat va avea dificultăți în a se adapta. În al doilea rând, tehnicile de învățare continuă, precum Elastic-weight Consolidation~\citep{kirkpatrick2017overcoming}, sunt concepute pentru tipuri incrementale de MTL. In aceste tipuri de MTL, etichetele de task sunt cunoscute, cantități mici de date anterioare pot fi încă disponibile pentru cazuri specializate~\citep{kirkpatrick2017overcoming} și pot exista diferite straturi finale ale modelului pentru fiecare task. Cerințele de confidențialitate ale FL fac ca astfel de soluții să fie dificile fără adăugarea unei memorii locale persistente.

\subsection{Eficiența Învățării Federate}
Trendurile pe care \citet{LargeCohorts} le-au descoperit au implicații majore asupra FL. Cele care limitează eficiența FL în setările Non-IID sunt de un interes deosebit. Se pot observa trei efecte semnificative. În primul rând, clienții foarte eterogeni pot provoca reduceri bruște ale acurateții când modelele lor sunt agregate. În al doilea rând, grupurile mai mari de clienți aduc îmbunătățiri diminuate pentru acuratețea finală și viteza de convergență a modelelor. În al treilea rând, grupurile mai mari scad eficiența sistemului, deoarece sunt necesare mai multe exemple pentru fiecare îmbunătățire a acurateții.

Aceste comportamente sunt analoge cu limitările bine-cunoscute de eficiență și generalizare a antrenamentului cu batch-size mari în ML~\citep{LargeBatchGenGapSharpMinima}. \citet{LargeCohorts} constată că problemele de eficiență sunt cauzate de scăderea mărimii pseudo-gradientului în mod proporțional cu creșterea dimensiunii grupului și de ortogonalitatea actualizărilor clienților. Autorii constată, de asemenea, că optimizatorii adaptivi sunt preferabili pe măsură ce dimensiunile grupurilor cresc datorită invarianței lor față de mărimea pseudo-gradientului.

\subsubsection{Optimizarea Federată Adaptivă}

De o relevanță specială pentru această propunere este Optimizarea Federată Adaptivă (FedOPT)~\citep{FedOPT}. FedOPT extind conceptul de optimizare adaptivă~\citep{Adam} la FL pe \textit{latura serverului}, tratând actualizările clienților ca pseudo-gradienți și păstrând informații în acumulatori între runde. Această structură permite ca FedOpt să minimizeze impactul rundelor individuale, agregând pseudo-gradienții lor cu cei ai rundelor anterioare. Deoarece rezultatul rundelor individuale este variabil în funcție de combinația de clienți selectați și de starea curentă a modelului, asemenea tehnici oferă o traiectorie de optimizare mai consecventă.

\begin{subequations}
    \begin{align}
        \Delta_t     & = \cfrac{1}{|C|} \sum_{c \in C} \left( \theta_t^c - \theta_t \right) \label{eq:FedOpt:line-2} \\
        m_t          & = \beta_1 m_{t-1} + (1-\beta_1) \Delta_t \label{eq:FedOpt:line-3}                             \\
        v_t          & = \beta_2 v_t + (1-\beta_2) \Delta_t^2 \label{eq:FedOpt:line-4}                               \\
        \theta_{t+1} & = \theta_t + \eta \cfrac{m_t}{\sqrt{v_t} + \tau} \label{eq:FedOpt:line-5}
    \end{align}
    \label{eq:FedOpt:all}
\end{subequations}

\noindent Urmând formularea din \citet{FedOPT}, precum în \cref{eq:FedOpt:all},pentru o anumită runda $t$ și model federat $\theta_t$, fiecare client $c$ în setul selectat $C$ antrenează local modelul pentru a construi o versiune personalizată $\theta_t^c$. Pseudo-gradientul $\Delta_t$ este apoi calculat drept mediei diferențelor dintre modelele personalizate și cel federat, precum în \cref{eq:FedOpt:line-2}. Operațiile pe tensori sunt element cu element, inclusiv împărțirea între tensori.

Acumulatorul mediei $m_t$ poate fi construit drept media ponderată a acumulatorului anterior $m_{t-1}$ și $\Delta_t$ folosind ponderea $\beta_1$, așa cum se arată în \cref{eq:FedOpt:line-3}. În mod similar, pentru versiunea FedOpt bazată pe Adam~\citep{Adam}, acumulatorul $v_t$  urmărește puterea a doua a fiecărui element a pseudo-gradientului, denotată prin $\Delta_t^2$, așa cum se arată în \cref{eq:FedOpt:line-4}. Aceste două acumulatoare sunt apoi folosite pentru a calcula modelul actualizat pentru runda următoare $\theta_{t+1}$, folosind rata de învățare a serverului $\eta$, precum în \cref{eq:FedOpt:line-5}. De remarcat, termenul $\sqrt{v_t}$ se referă la rădăcina pătrată a fiecărui element. Termenul este utilizat pentru a face algoritmul invariant față de mărimea pseudo-gradientului. În final, $\tau$ controlează adaptivitatea FedOPT\@.

FedOPT prezintă mai multe proprietăți promițătoare în contextul FL ierarhic. În primul rând, \citet{FedOPT} arată că este foarte robust la alegerea exactă a hiperparametrilor, inclusiv a ratei de învățare, în comparație cu FedAvg. În al doilea rând, invarianța algoritmului abordează parțial problemele observate de \citet{LargeCohorts} cauzate de pseudo-gradienții apropiați de zero. În al treilea rând, ei oferă un mijloc de a diferenția automat ratele de învățare ale mai multor servere în funcție de starea acumulatoarelor lor.

\subsection{Lucrări corelate}

Pentru a aborda compromisul dintre optimizarea pentru performanța globală și pentru performanța pe datele unui client specific, ce poate fi observat în \cref{eq:flObjective}, două direcții generale au apărut în literatura științifică. Prima, exemplificată de ``Fair'' FL~\citep{QFedAvg}, încearcă să modifice importanța unui client în funcția obiectiv federată pentru a schimba eficiența modelului final pentru acel client. A doua relaxează cerința unui singur model global prin: personalizarea modelului federat~\citep{SalvagingFL,FLwithNonIID}, menținerea unui model local persistent~\citep{Ditto}, gruparea clienților pe baza similitudinii~\citep{ThreeApproachesMansour} și construirea ierarhiilor~\citep{Client-Edge-CloudHierFL,Hier_Het_Cellular}. Deoarece familia propusă de algoritmi B-HFL se încadrează în a doua categorie, această secțiune va detalia lucrările strâns legate și limitările acestora. În final, proprietățile dorite ale B-HFL sunt rezumate în Tabelul \ref{tab:gap_analysis}.


\subsubsection{Învățare Federată Personalizată}

Învățarea Federată complet personalizată creează un model adițional pentru fiecare client. Cea mai comună metodă este personalizarea prin adaptare locală (fine-tuning), a modelului federat după antrenament~\citep{SalvagingFL}, cu posibile tehnici precum Învățare Mutuală~\citep{DeepMutualLearning} sau Elastic-weight Consolidation~\citep{kirkpatrick2017overcoming}. Cu toate acestea, optimizarea în două etape este dificil de implementat într-un ciclu de viață FL unde modelul federat poate necesita un antrenament suplimentar ulterior adaptării. Mai mult, nu oferă un punct de mijloc între modelele globale și cele locale, ceea ce afectează capacitatea unor astfel de sisteme de a integra noi clienți.

O abordare mai recentă este reprezentată de Ditto~\citep{Ditto} pentru setări în care clienții sunt vizitați frecvent și pot menține starea între runde. Ditto permite clienților să mențină un model local persistent și să-l antreneze alături de cel federat în timpul rundelor FL. Cele două modele sunt conectate prin includerea distanței $l_2$ dintre parametrii lor în funcția de cost a modelului local. Cu toate acestea, în ciuda beneficiilor dovedite de ``fairness'' și ``robustness'', Ditto nu abordează modificările setului de date în cadrul clientului, deoarece modelele funcționează numai în timpul rundelor de antrenament.

\subsubsection{Învățare Federată Ierarhică și Clustering}

Subdomeniul FL cel mai relevant pentru propunerea aceasta este Învățarea Federată Ierarhică (referită ca HFL din termenul englez ``Hierarchical Federated Learning'') introdusă de \citet{Client-Edge-CloudHierFL}. Algoritmul propus (HierFAVG) a fost dezvoltat în principal pentru a gestiona provocările de comunicare ale sistemelor FL anterioare. Pentru a gestiona milioane de clienți participanți~\citep{GoogleKeyboard, ScaleSystemDesign}, sistemele FL se bazau pe infrastructura cloud, având viteze de comunicare reduse, pentru a conecta dispozitivele pe o zonă geografică largă. Acest compromis a fost dorit, deoarece populațiile mari erau necesare pentru convergență, iar serverele edge, deși capabile de comunicare rapidă cu clientul, nu puteau să obțină suficienți clienți. \citet{Client-Edge-CloudHierFL} susțin că o structură pe două niveluri rezolvă tensiunile între serverele edge și serverele cloud. \citet{Hier_Het_Cellular} propun un algoritm identic pentru rețelele celulare eterogene. Similar cu HierFAVG~\citep{Client-Edge-CloudHierFL}, \citet{Hier_Het_Cellular} se axează pe reducerea costurilor de comunicare utilizând tehnici de compresare a pseudo-gradienților~\citep{DeepGradientCompressin}.

Clusterizarea clienților este o tehnică sinergică ce încearcă să grupeze participanții pe baza unei metrice de similaritate. Aceste clustere sunt construite folosind abordări precum clusterizarea directă a parametrilor modelului~\citep{ClusterFL} sau utilizarea valorii funcției de cost a clienților atunci când sunt atribuiți unui anumit cluster~\citep{ThreeApproachesMansour}. Clusterele pot exista și natural pe baza unor caracteristici precum locația geografică sau limba.

Lucrările anterioare în HFL arată o serie de limitări. Algoritmul HierFAVG extinde direct FedAvg~\citep{FedAvg} permițând serverului cloud să trateze serverele edge ca pe clienți. Cu toate acestea, deoarece \citet{Client-Edge-CloudHierFL} și \citet{Hier_Het_Cellular} iau în considerare doar eficiența comunicației, ei nu permit serverelor edge să mențină o personalizare mai mare și, în schimb, le înlocuiesc complet modelul în timpul agregării în cloud. În plus, sistemul lor nu ia în considerare asincronia, antrenamentul pe seturi de date proxy sau ierarhiile multi-nivel. În ceea ce privește clusterizarea, algoritmii disponibili nu reușesc să obțină compromisul dorit între generalizare și personalizare. Algoritmii de clusterizare standard în FL presupun comunicarea parametrilor între clustere a fi inutilă și nu se mapează direct pe o structură de comunicare ierarhică. Mai mult de atât, acești algoritmi nu sunt meniți să ofere un singur model global în afara modelelor de clustere.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
% \usepackage[normalem]{ulem}
% \useunder{\uline}{\ul}{}
\begin{table}[h]
    \centering
    \caption{Tabel de analiză ce arată proprietățile sistemului propus și suprapunerea cu lucrările corelate.}
    \resizebox{\columnwidth}{!}{%
        \begin{tabular}{@{}lcccccc@{}}
            \toprule
            Lucrări Corelate                           & Structură Ierarhică             & Personalizare                   & Permite Modele Persistente      & Modele Generale de Grup         & Modele Semnificative de Grup    & Antrenament Asincron            \\ \midrule
            Adaptare Locală                            &                                 & {\color{ForestGreen}\checkmark} &                                 &                                 &                                 &                                 \\
            Ditto                                      &                                 & {\color{ForestGreen}\checkmark} & {\color{ForestGreen}\checkmark} &                                 &                                 &                                 \\
            Clustering                                 &                                 &                                 &                                 &                                 &                                 &                                 \\
            HieFAVG                                    & {\color{ForestGreen}\checkmark} &                                 &                                 & {\color{ForestGreen}\checkmark} & {\color{ForestGreen}\checkmark} &                                 \\
            FL Asincron                                &                                 &                                 &                                 &                                 &                                 & {\color{ForestGreen}\checkmark} \\
            Învățare Federată Ierarhică Bidirecțională & {\color{ForestGreen}\checkmark} & {\color{ForestGreen}\checkmark} & {\color{ForestGreen}\checkmark} & {\color{ForestGreen}\checkmark} & {\color{ForestGreen}\checkmark} & {\color{ForestGreen}\checkmark} \\ \bottomrule
        \end{tabular}%
    }\label{tab:gap_analysis}
\end{table}
