The proposal in this document emerged as a natural consequence of research on Personalised Federated Learning and Hierarchical Federated Learning I began during my MPhil in Advanced Computer Science and the first year of my PhD in the Cambridge ML Systems lab led by my supervisor Dr.\ Nicholas Lane.

\citet{EuroMLSysWorkshop} investigated the trade-off between generalisation and personalisation, which is at the heart of this work, from the perspectives of Fair Federated Learning and its interactions with local adaptation~(fine-tuning) of the federated model post-training. Since Fair Federated Learning attempts to construct a more uniform accuracy distribution for the federated model over the local test sets of clients, the expectation was to either reduce the need for personalization or to provide a better starting point from which to carry it out. The experimental results showed that Fair FL brings no benefits and potential downsides towards later personalization and led to the proposal of a Personalisation-aware FL algorithm that attempts to anticipate the common regularises used during fine-tuning throughout the FL process.

\citet{OperaWorkshop} evaluated the performance of Federated Human Activity Recognition~\citep{HARusingFL_2018} when trained using multimodal data gathered from different sensor types at increasing levels of privacy. It showed that grouping clients based on the type of sensor that produced their training set effectively mitigated the impacts of privacy being required at a human subject, environment, and sensor level simultaneously. It was a direct precursor to Bidirectional Hierarchical Federated Learning as it relied on a two-tiered model structure where each client trained both a group-level model and the global federated model using a mutual learning approach~\citep{DeepMutualLearning}. This work was later extended to consider the adaptability of such two-tiered systems to the addition of a new sensor type~(group) into the federated; the extension was submitted to the \href{https://mobiuk.org/2023}{MobiUK} symposium. Mutual learning was chosen to relate the group-level and global models since it allowed divergent architectures that only shared the output layer. However, despite its success, this training method requires clients to have a high amount of data and local epochs to train both models. The expensive nature of the procedure prompted a move towards a model-averaging approach.

Both of the previous works were implemented in the Flower~\citep{Flower} FL framework; however, the scale of experimentation required for fully validating B-HFL would be unfeasible on the publicly available simulation engine. As such, I have contributed to constructing a new engine that doubles Flower simulations' throughput by intelligent ML-based client placement on GPUs. The paper presenting our techniques,for which I share an equal-contribution credit as a primary author, ``High-throughput Simulation of Federated Learning via Resource-Aware Client Placement'' has been submitted to \href{https://sigmobile.org/mobicom/2023/}{Mobicom} and is pending review.


