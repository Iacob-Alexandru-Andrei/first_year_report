Given the shortcomings of traditional hierarchical FL systems, this chapter proposes Bidirectional Hierarchical Federated Learning (B-HFL), an alternative family of methods that optimises data and communication efficiency while allowing flexible degrees of personalisation. \cref{sec:proposal:research_directions} then presents the future research directions of the PhD enabled by B-HFL\@.

\section{Research Questions}\label{sec:proposal:research_questions}
The proposed research aims to answer the following research questions during the PhD\@:
\begin{singlespace*}
    \begin{enumerate}
        \item Can multiple servers with small cohorts \emph{outperform large-cohort single-server FL by avoiding the data efficiency problems identified by \citet{LargeCohorts}?}
        \item If the need for convergence to a global model is removed, \emph{can hierarchical FL effectively address the trade-off between generalisation and personalisation with Non-IID data?}
        \item If all nodes are treated homogeneously, and servers are allowed to train on proxy data, \emph{can the regularisation strength be more effectively controlled with the hierarchical structure?}
        \item Can the network structure be used to \emph{enhance or design aggregation strategies without major harm to communication efficiency?}
        \item Do persistent client models with continual asynchronous learning allow nodes to \emph{tackle dataset shift and obtain a greater degree of personalisation?}
    \end{enumerate}
\end{singlespace*}
All of these questions are intertwined with the hierarchical structure itself, and, unlike previous work in hierarchical FL, they are all predicated on the abandonment of a single global model as the explicit goal of FL\@. Consequently, they can all be reduced to the following question: \emph{Can a hierarchical FL structure allow us to better utilise node data on both clients and servers while maintaining the communication efficiency which made FL practical in the first place?}

As shown in \cref{tab:gap_analysis} and discussed in \cref{sec:back:related_work}, previous approaches in the field are not flexible enough to simultaneously allow trade-offs in terms of personalisation, sample efficiency, communication efficiency and asynchronous training or execution. As such, research in the following proposed family of algorithms would significantly contribute to the field.
\section{System Design}\label{sec:proposal:system_design}

Bidirectional Hierarchical FL organises communication between servers and controls the dissemination of training parameters through the following design choices:
\begin{singlespace*}
    \begin{enumerate}
        \item While previous methods such as HierFAVG~\citep{Client-Edge-CloudHierFL,Hier_Het_Cellular} entirely replace the edge-server and client models after global aggregation takes place, B-HFL performs partial aggregation between a child node and their parent. This
              allows children to maintain their local weights while incorporating global information. It proceeds in two phases:

              \begin{enumerate}
                  \item \textbf{Leaf-to-root aggregation} After clients finish training, their information is propagated up the tree. Each internal node has a parameter $T_n$, which determines after how many rounds it sends its updates to the parent. This value is equivalent to local client epochs during SGD and may be the same for a tree level or independent per node.
                  \item \textbf{Root-to-leaf aggregation} After a node has received and aggregated the training result from some or all of its children, it propagates its parameters down their subtree. The propagation cost is proportional to the depth; however, communication between internal nodes can be assumed to be faster than between clients and edge servers.
              \end{enumerate}

        \item All nodes may be allowed to execute synchronously or asynchronously concerning other nodes on the same level if necessary during leaf-to-root aggregation. For leaves (clients), this is equivalent to traditional asynchronous FL~\citep{AsynchronousFLonHetDevicesSurvey}. For an internal node, the same federated asynchronous strategies~\citep{FedBuff,PAPAYA} can be applied when receiving models from children, with client training replaced by executing the subtree rooted at the child node.

        \item Internal nodes within the hierarchical structure can train on proxy datasets to regularise training as done by \citet{OneShotFL,FLwithNonIID}. Proxy training is especially relevant for language modelling as large public corpora are available. In order to avoid operating on stale parameters, the natural point for such training is after leaf-to-root aggregation and before root-to-leaf aggregation. However, the latency incurred from such training may be too large. In that case, parameters may train asynchronously while the subtree executes.
    \end{enumerate}
\end{singlespace*}
Thus, the objective function of FL from \cref{eq:flObjective} is modified for B-HFL as described in \cref{eq:BHFL:all}

\begin{subequations}
    \begin{align}
        \underset{\theta}{\min} F_q(\theta) & = \alpha_q f_q(\theta) + \beta_q \, F_{D_q}(\theta) + \gamma_q \, F_{A_q}(\theta) \\
        F_{D_q}(\theta)                     & = \sum_{d \in D_q} p_d F_d(\theta)                                                \\
        F_{A_q}(\theta)                     & = \sum_{a \in A_q} p_a F_a(\theta)                                                \\
        f_q(\theta)                         & = \cfrac{1}{\lvert \Omega_ \rvert} \underset{j \in \Omega_q}{\sum} f_q^j(w)
    \end{align}
    \label{eq:BHFL:all}
\end{subequations}
where each node $q$ in the tree attempts to find the model $\theta$ which minimizes its local objective $f_q$, that of its descendants $F_{D_q}$, and ancestors $F_{A_q}$ using weights $\alpha_q,\beta_q,\gamma_q$. The objective of the descendants and ancestors are recursively described, while the local objective $f_q$ is defined by the performance of the model $\theta$ on the local node dataset $\Omega_q$. In the case of a leaf node, only its local objective and that of the ancestors matter, while for the root, only its local objective and that of the descendants matter. If an internal node lacks a proxy dataset, only $F_{D_q}$ and $F_{A_q}$ are optimised. All leaf nodes are expected to have local datasets.

Expressly, parameters aggregated from the leaf nodes (clients) up through the tree are fine-tuned to relevant local data. In contrast, parameters transmitted from parents to children are averaged over more numerous populations. When servers cover meaningfully clustered clients, these populations may be less related (e.g., covering multiple languages). Furthermore, if internal nodes are allowed to train on proxy datasets, they inject additional training into the federated models and provide regularisation for the entire tree. In traditional FL approaches, training on the server directly controlling the clients can impose overly strong regularisation; however, in B-HFL, higher nodes in the tree already represent a global picture and have limited impact at the leaves as their influence gets diluted through multiple intermediary nodes. Finally, allowing each client to maintain a persistent model across rounds and aggregate with their parents rather than entirely replacing their model makes them identical to any other node except for not having children. Keeping persistent models and repeatedly re-aggregating them is roughly analogous to the Iterative Moving Averaging applied by \citet{UnderstandingModelAveragingInFL}.

Since not all nodes in the tree are required to be capable of training, it is worth distinguishing models which have been optimised via additional learning rather than mere aggregation. Specifically, training data being available may enable more efficient learning-based aggregation methods such as mutual learning~\citep{DeepMutualLearning}, $l_2$-based regularisation~\citep{Ditto} or model interpolation with adaptive weights~\citep{AdaptivePersonalisedFederatedLearning,ThreeApproachesMansour}. Additionally, updates constructed via training directly may offer a better optimisation signal, similarly to methods trying to build diverse ensembles~\citep{StochasticMultipleChoiceLearningDiverseEnsembles} or diverse models for parameter averaging~\citep{DiverseWeightAveraging}. Thus, B-HFL proposes adding dataflows directly between training nodes (e.g., clients and the root) while using the underlying hierarchical communication structure, like the ``residual'' connection in ResNet~\citep{ResNet}. For example, the system could allow the $K$ client updates of each server with the highest absolute value to pass all the way to the root, where they may be merged via either training or adaptive optimisation with independent accumulator states. This sort of vertical connection provides highly dynamic and potentially cyclic dataflow. Another avenue worth exploring is allowing nodes, especially clients, to train asynchronously using their persistent model. This would permit clients to account for local dataset shifts using well-known techniques from the Continual Learning literature~\citep{ContinualLearningSurvey,LearningWithoutForgetting,EWC}.

\cref{alg:B-HFL} describes B-HFL recursively starting from the system's root. It assumes that the $\textsc{Train}$ and node aggregation $\textsc{NodeOpt}$ procedures are provided. All variables are indexed per node and assumed to be provided by the implementation. The ``residual'' connections are adjacency lists between nodes and their ancestors/descendants in $\mathrm{AncRes}/\mathrm{DescRes}$.

\begin{algorithm}[H]
    \caption[Bidirectional Hierarchical FL]{Recursive algorithm for a generic version of B-HFL\@. Each node $q \in Q$ has an associated persistent model $W_q$, number of rounds $T_q$, child nodes $C_q$, leaf-to-root learning rate $\eta^\uparrow$, root-to-leaf learning rate $\eta^\downarrow$. ``Residual'' edges are kept between nodes and their ancestors/descendants in $\mathrm{AncRes}/\mathrm{DescRes}$
        with models accumulated in the lists $R^\uparrow$ and $R^\downarrow$. }\label{alg:B-HFL}
    \begin{onehalfspace}

        \begin{algorithmic}[1]
            \State \textbf{Require} \(Q, W, T, C, \eta^\uparrow, \eta^\downarrow, \eta^l, \Omega\) \label{alg:B-HFL:line:r0}
            \Comment{lists indexed over all the nodes in Q}
            \State \textbf{Require}  $R^{\uparrow},R^{\downarrow}$ \Comment{list of lists of models that a node $q$ receives from children/ancestors} \label{alg:B-HFL:line:r1}
            \State \textbf{Require}  $\mathrm{AncRes},\mathrm{DescRes} $ \Comment{list of ``residual'' connections to descendants/ancestors} \label{alg:B-HFL:line:r2}
            \State \textbf{Require}  \(\textsc{Train} ,\textsc{NodeOpt}, \textsc{SelectResiduals}  \) \label{alg:B-HFL:line:r3}

            \Procedure{ExecuteNode}{$\phi, q$}  \label{alg:B-HFL:line:1}
            \If{$q = \emptyset$}
            \textbf{return} $\emptyset$ \Comment{error checking} \label{alg:B-HFL:line:2}
            \EndIf
            \State $\theta_0 \gets W_q$  \Comment{handle root} \label{alg:B-HFL:line:3}
            \If{$\phi \neq \emptyset$} \label{alg:B-HFL:line:4}
            \State $\theta_0 \gets \Call{NodeOpt}{q,W_0,[\phi], R^\downarrow_q, q, \eta^\downarrow_q} $ \Comment{aggregate parent $[\phi]$ and ``residuals''} \label{alg:B-HFL:line:5}
            \EndIf

            % \State $\Delta_0 \gets W_q - \phi$


            \ForEach{ round $t \gets 0, \ldots, T_q-1$} \label{alg:B-HFL:line:6}
            \ForEach{ node $d \in \mathrm{DescRes}_q$}  \label{alg:B-HFL:line:8}
            \State $ R^\downarrow_d \gets [\theta_t]$ \label{alg:B-HFL:line:9}
            \EndFor
            \State $S \gets $Sample a subset from $q$'s set of children $C_q$ \label{alg:B-HFL:line:10}


            \ForEach{ node $c \in S$} \label{alg:B-HFL:line:11}
            \State $\theta_t^c \gets \Call{ExecuteNode}{\theta_t, c} $ \Comment{sync/async} \label{alg:B-HFL:line:12}

            % \State $\Delta_t^c \gets \theta_t^c - \theta_t$
            \EndFor

            \ForEach{ node $a \in \mathrm{AncRes}_q$} \label{alg:B-HFL:line:13}
            \State $ R^\uparrow_a \gets \Call{SelectResiduals}{q,[\theta^c_t \,\, \forall c \in S]}$ \label{alg:B-HFL:line:14}
            \EndFor
            % \State $\Delta_t \gets \frac{1}{|C_q|} \sum_{c \in C_q} \Delta_t^c$
            \State $\theta_t^\prime \gets \Call{NodeOpt}{q,\theta_t, [\theta^c_t \,\, \forall c \in S], R^\uparrow_q, \eta^{\uparrow}_q}  $ \Comment{aggregate children and ``residuals''} \label{alg:B-HFL:line:15}
            \State $\theta_{t+1} = \Call{Train}{\theta_{t}^\prime,\Omega_q, \eta^l_q}$ \Comment{train (sync/async) parameters on node data} \label{alg:B-HFL:line:7}
            \EndFor
            \State $W_q \gets \theta_{T_q}$ \Comment{update persistent node model} \label{alg:B-HFL:line:16}
            \State \textbf{return} $\theta_{T_q}$ \label{alg:B-HFL:line:17}
            \EndProcedure

            \State $\Call{ExecuteNode}{\phi = \emptyset, q = root}$
        \end{algorithmic}
    \end{onehalfspace}
\end{algorithm}
\noindent In its natural language form, \cref{alg:B-HFL} operates as follows:
\begin{singlespace*}
    \begin{enumerate}
        \item For the root, use the persistent model as the initial federated model $\theta_0$.  [\cref{alg:B-HFL:line:3}]

        \item \textbf{Root-to-leaf aggregation:} Use$\textsc{NodeOpt}$ to aggregate the persistent node model with the parent model $\phi$ and those in ``residual'' connections from ancestors $R^\downarrow_q$ using $\eta^\downarrow_q$. [\cref{alg:B-HFL:line:5}]
        \item Begin executing federated rounds. [\cref{alg:B-HFL:line:6}]

        \item Add ancestor model $\theta_t$ to $R^\downarrow_d$ for descendants with ``residual'' connections. [\cref{alg:B-HFL:line:8} to \ref{alg:B-HFL:line:9}]
        \item Sample node subset $S$ for execution. For edge servers, $\lvert S \rvert$ would equal the client cohort size. For non-edge servers, $S = C_q$ while for a leaf node~(client) $S = \emptyset$. [\cref{alg:B-HFL:line:10}]
        \item Recursively execute nodes in the subtree of selected children, sending $\theta_t$.  [\cref{alg:B-HFL:line:11} to \ref{alg:B-HFL:line:12}]
        \item Select and send children models $\theta_t^c$ to $R^\uparrow_a$ for ancestors with ``residual'' connections. [\cref{alg:B-HFL:line:13} to \ref{alg:B-HFL:line:14}]
        \item \textbf{Leaf-to-root aggregation:} Use$\textsc{NodeOpt}$ to aggregate $\theta_t$ with the children models $[\theta^c_t \,\, \forall c \in S]$ and those in ``residual'' connections from descendants $R^\uparrow_q$ using $\eta^\uparrow$.  [\cref{alg:B-HFL:line:15}]
        \item Train $\theta_t$ on the potentially empty dataset $\Omega_q$ using local learning rate $\eta^l_q$. \textit{This is where edge clients and servers with proxy datasets would execute training.} [\cref{alg:B-HFL:line:7}]
        \item After federated training, update the persistent model $W_q$ with the most recent federated model $\theta_{T_q}$ and then return $\theta_{T_q}$.[\cref{alg:B-HFL:line:16} to \ref{alg:B-HFL:line:17}]
    \end{enumerate}
\end{singlespace*}
The synchronicity of $\textsc{Train}$ is defined concerning the execution of child nodes. If training is synchronous, it must complete before child nodes begin execution. If async, the model sent to a child would be $\theta_{t}^\prime$ prior to training, and the post-training $\theta_{t+1}$ would be used during leaf-to-root aggregation if completed. When async training is used, it must be accounted for during the aggregation procedure with a potential staleness factor.

``Residual'' connections from descendants to ancestors may send multiple children models (e.g., the $K$ models representing the largest updates) directly or after a ``residual'' aggregation procedure which merges them. On the other hand, ``residual'' connections from ancestors to descendants only need to send one model. A relevant example of a $\textsc{NodeOpt}$ procedure is FedOPT~(\cref{eq:FedOpt:all})~\citep{FedOPT}. FedOPT can be adapted to handle ``residual'' connections by adding a second accumulator state and averaging the input from the ``residuals''.
Bidirectional Hierarchical FL  may bring several potential benefits:
\begin{enumerate}
    \item It can accommodate nodes with different aggregation methods, learning rates, and dynamic optimiser states for leaf-to-root and root-to-leaf aggregation. Similarly to the number of rounds, aggregation parameters may be independent or set per tree or level.
    \item Using small cohorts for edge-servers avoids the issue of decreasing pseudo-gradients norms noticed by \citet{LargeCohorts}, as does clustering clients during edge-server assignment.
    \item While persistent local models are known to work well in cross-silo FL, this hierarchical structure makes them relevant in cross-device settings by potentially allowing a more significant number of clients to be sampled per round, thus visiting them more than once.
    \item Can naturally integrate Secure Aggregation~\citep{SecAggOG,FastSecAgg} at the level of each edge-server. As first noted by \citet{ScaleSystemDesign}, this reduces the additional communication cost of training $C$ clients with Secure Aggregation from $\mathcal{O}(C^2)$ to $\mathcal{O}(C^2/M)$, where $M$ is the number of edge-servers. Secure Aggregation and Differential Privacy~\citep{DiffPrivacyFL} only need to be applied at the lowest level of the tree.
\end{enumerate}

\section{Example System}\label{sec:proposal:example_system}
\begin{figure}[h]
    \centering
    \includegraphics[clip,width=\columnwidth]{plots/Tree_Structure.drawio.pdf}
    \caption[System Diagram]{Example B-HFL system. Solid lines represent communication links, and dashed lines represent conceptual ``residual'' connections. Nodes capable of training, such as clients and the central server with proxy data, are in grey. When model parameters propagate up, nodes merge incoming pseudo-gradients and update their model with learning rate $\eta^\uparrow$. The same happens when parameters flow from parents to child nodes with learning rate $\eta^\downarrow$. Since the dashed lines communicate $0$ to $K$ models, $\eta^\Uparrow$ may represent $0$ to $K$ aggregations using a $\eta^\uparrow$ learning rate.}\label{fig:TreeStructure}
\end{figure}

An example of a B-HFL system, which would be the first deliverable for the second year of my PhD, may be seen in \cref{fig:TreeStructure}. The central server controls a proxy dataset used to train after it performs aggregation. Intermediary servers perform only aggregation. Servers send updates to the parent after every round.

Each node, including the clients, runs at-least two stateful FedOPT server optimisers with separate learning rates, one for the leaf-to-root aggregation and one for parent aggregation. Even if the same leaf-to-root learning rate $\eta^\uparrow$ and root-to-leaf learning rate $\eta^\downarrow$ were to be used for all nodes in the tree or at a given level, the independent server optimiser states would distinguish the aggregation procedure of their node based on historical trends. The central server uses model interpolation with a mixture parameter~\citep{AdaptivePersonalisedFederatedLearning} adapted to its proxy data to merge child updates.

The ``residual'' connections serve different functions between the leaf-to-root and root-to-leaf stages. For the upward stage, they collect the $K=1$ client update with the highest absolute value, thus sending one additional model to the central server per edge server. For the downward stage, they allow the edge servers to directly benefit from the central server's training without relying on averaged intermediate models. While this last component is somewhat superfluous in the small hierarchy shown by \cref{fig:TreeStructure}, it may prove relevant for profound structures. For example, in deep hierarchies, parameters that receive extra training at the central server might get averaged several times before reaching the edge servers and thus influencing the leaves.


\section{Research Directions}\label{sec:proposal:research_directions}
\cref{alg:B-HFL} imposes sufficient structure to create a new family of hierarchical FL systems, opening a series of research directions. These directions can be primarily divided into three types: (a) node aggregation procedures filling in the \textsc{NodeOpt} function, (b) ``residual'' selection procedures filling in the \textsc{SelectResiduals} function, and (c) clustering algorithms which decide how client nodes are divided across the physical or virtual edge-servers. Both node aggregation and ``residual'' selection are expected to be set for types of nodes or levels of the tree, as allowing each node to have a separate aggregation procedure would be difficult to manage. Regarding clustering, relations imposed by the physical communication links and purely conceptual ones must be distinguished as the former are unalterable. Importantly, a physical edge server may contain multiple virtual nodes to which clients may be assigned.

\paragraph{Node Aggregation Procedures} Relevant node aggregation procedures can either be those developed for standard FL or procedures that take advantage of the unique tree structure or available proxy data. FedOPT~\citep{FedOPT} and Iterative Moving Averaging~\citep{UnderstandingModelAveragingInFL} are examples of standard FL algorithms that offer unique \emph{implicit} benefits for this hierarchical structure because they maintain stateful accumulators or previous models, respectively, which permit every server to be distinguished. The smaller number of children of non-edge servers may also enable costly aggregation procedures.  For example, internal nodes having potential proxy data allows them to use model interpolations via either training regularisation~\citep{Ditto} or by using adaptive interpolation rates, as proposed by \citet{AdaptivePersonalisedFederatedLearning}, optimised to the proxy data in order to balance the influence of updates from child nodes, parents nodes, and
from proxy training. Aggregation procedures can also \emph{explicitly} consider the links between nodes. A simple example of this second type is aggregation considering the distance between an ancestor and a ``residual'' descendant. More complex procedures may combine the known hierarchical topology with similarity metrics to create a distance matrix between internal node models and perform graph message passing~\citep{GNNSurvey}, as proposed by \citet{FederatedLearningWithGraph}, to interpolate parameters between all internal nodes. A well-known example of this message-passing aggregation style is the previously discussed FedAMP~\citep{FedAMP}, whose constraint of having to maintain additional personalised models on the server is less prohibitive for an internal node with few children.
\paragraph{``Residual'' Selection Procedures} For ``residuals'' to be beneficial during aggregation, they must contain information not already evident in their parent's model. As mentioned, it is well-known that averaging is an imperfect means of aggregating models trained on Non-IID data~\citep{FedAvg,LargeCohorts,OnTheConvergenceOfFedAvgOnNonIIDdata,FedProx} as the directions of different pseudo-gradients may conflict. Additionally, maintaining some degree of diversity in ensembles~\citep{StochasticMultipleChoiceLearningDiverseEnsembles} and parameter averages~\citep{DiverseWeightAveraging} is known to be potentially beneficial. As such, in the case of leaf-to-root ``residuals'', parameters may be selected based on simple metrics like an $l_n$ norm, their per-sample loss, or their relation to each other (e.g., cosine similarity). If necessary for communication efficiency, an average update from the top-K outlier gradients, according to the metric, may substitute selecting multiple ``residuals''.
\paragraph{Clustering Algorithms}  Since the connection between cloud and edge servers or multiple layers of intermediary servers is fixed by physical communication links, the underlying communication hierarchy must also remain fixed, making edge reassignments impossible. However, each physical server can create arbitrarily many internal virtual nodes if it adheres to the structure in \cref{alg:B-HFL}. Thus, each physical server is assumed to have the ability to create internal node hierarchies, and edge servers are assumed to have the ability to create clusters for clients within their geographic area. These properties could be exploited for the creation of a B-HFL dynamic clustering system similar to Auxo~\citep{Auxo} where edge-servers would split into multiple virtual nodes to create clusters for clients to be assigned to; however, unlike Auxo, these clusters would maintain a common parent node and thus share information.
\paragraph{Criteria for Success} Given these research directions, it is essential to consider their impact. The proposed research will be successful in the near term if it leads to published work based on the example system in \cref{sec:proposal:example_system}, at ICLR, MLSys, NeurIPS, or an equivalent conference, as detailed in \cref{sec:timeplan:secondYear}. Within the scope of the whole PhD, B-HFL would be successful if its structure allows for faster convergence to the global optimum by avoiding the diminishing effects of large cohorts, creating a more performant global model, or constructing excellent cluster models through inter-cluster communication. Alternative criteria include achieving similar performance to standard FL algorithms with lower communication costs or obtaining evidence that ``residual'' connections and bidirectional partial aggregation are independently beneficial.
\paragraph{Potential Risks} Considering the previously mentioned criteria, potential risks to the methodology include: (a) the continued proliferation of large models trained on public corpora, which are challenging to federate without extensive GPU resources being available to the clients, (b) a lack of adaptivity in the hierarchical structure. For the first risk, if large models continue to proliferate, efficiently training them in a federated fashion via model splitting would need to be added as a primary research direction. For the second, if the proposed B-HFL structure is too rigid to manipulate and would be thus difficult to apply in a practical FL scenario, it would be preferable to refocus the work entirely on inter-cluster communication through a parent node where the depth of the hierarchy never exceeds that of the example system in \cref{fig:TreeStructure}.


